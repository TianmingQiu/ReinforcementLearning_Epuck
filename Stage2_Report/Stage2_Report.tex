%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TU Muenchen 17 Sommer Semester 
% Applied Reinforcement Learning
% 2nd Report 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)
\usepackage{varioref}
\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images
\usepackage{booktabs}
\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Finding the Shortest Path Using Reinforcement Learning}} % Subtitle

\author{\textsc{Lingfeng Zhang, Wenhan Hao, $\&$ Tianming Qiu} % Author
\\{\textit{}}} % Institution

\date{Group: applied-rl17 / E3} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section



%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section{Consice Introduction of Project}
This project focuses on finding a shortest path in a map with many cross roads. The agent has to make an optimal decision to turn into a correct or best direction at each road crossing, namely the state in terms of Markov Decision Process.

%------------------------------------------------

\section{Markov Decision Process Description}

Obviously, the states in this model are the different road crossings. State transition, namely the transition of different road crossing, is adopted as a stochastic process. The modelling of this issue satisfies the Markov Decision Process (MDP). 
First, it has the Markov property. And the transition from road crossing $C_{t}$ to next crossing $C_{t+1}$ is independent of all the previous states and actions. No matter which road crossing the Epuck was in or which actions it takes before the road crossing $C_{t}$, the probability that it transits from current state to next state would be never influenced. In another words, it can be represented by: $P(s_{t+1})=P(s_{t},a_{t})$. The current state captures all that is relevant about the world in order to predict what the next state will be.
Secondly, it is clear to see that the state transition is related to action. Epuck can choose to turn right, turn left, go ahead or backward, and the consequence will be dependent on the chosen actions at current state. 

A Markov decision process is a discrete-time state transition system. It can be described formally with 5 components as a 5-tuple $(\mathcal{S,A,P}, r,\gamma)$ as below:

\textbf{$\mathcal{S}$: States} First, it has a finite set of states. These states will play the role of outcomes in the decision theoretic approach, as well as providing whatever information is necessary for choosing actions. 

\textbf{$\mathcal{A}$: Action}
A small finite set includes turn left, turn right, go forward and backward.

\textbf{$\mathcal{S}$: Transition probability}
The transition probabilities describe the dynamics of the world. They play the role of the next-state function in a problem-solving search, except that every state is thought to be a possible consequence of taking an action in a state. So, we specify, for each state $s_{t}$ and action $a_{t}$, the probability that the next state will be $s_{t+1}$. 
Now we just model the transition probability as the simplest one: All of them equal to one. That means if and only if Epuck takes a specific action (e.g. turn right) at the current action, it will transit into a definitely deterministic state.

\textbf{$r$: Reward} In this model we will assign a reward '+10' at the destination state and '0' at the other states. 
Also at the 'blind alley' it will get a very poor reward such as '-100'.

\textbf{$\gamma$}  $\gamma$ is a discout factor here we select as 0.9.


%------------------------------------------------
\section{Algorithm}
In recent years, reinforcement learning has achieved many impressive results, including playing video games, and training advanced manipulation skills. The goal of this experiment is to realize artificial agents, which can be trained and can find the shortest path in intricate traffic environment. Here we consider using An off-policy TD control algorithm: Q-learning to achieve our goal.


We define a discrete-time finite-horizon discounted Markov decision process (MDP) by a tuple $(\mathcal{S,A,P}, r,\gamma)$. The agent interacts with an environment over a time of discrete time steps. At each time step, the agent receives a state from $\mathcal{S}$ and select an action from $\mathcal{A}$ according to its policy $\pi$, where $\pi$ is a mapping from states $\mathcal{S}$ to action $\mathcal{A}$. In return, the agent receives the next state and a scalar reward $r$. The process continues until the agent reaches a terminal state. The total return is the accumulated reward with discount factor $\gamma$. The action value $Q^{\pi}(s,a)$ is the expected return for selecting action a in state s and following policy $\pi$. The optimal value function $$Q^*(s,a)=\max \limits_{\pi}Q^{\pi}(S,a)$$ gives the maximum, action value for state s and action a achievable by any policy.

In Q-learning, the action value $Q(s,a)$ will be updated toward one-step return as below:$$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+ \alpha [ R_{t+1} + \gamma \max \limits_{a}Q(S_{t+1},a)-Q(S_t,A_t)]$$

Each time obtaining a reward $r$ can directly affects the value of the state action pair $s,a$ that led to the reward. The pseudocode of Q-learning algorithm is shown below:
\begin{center}\textbf{Q-learning: An on-policy TD control algorithm}

\fbox{\parbox{\textwidth}{Initialize $Q(s,a)$, $\forall s \in \mathcal{S}$, $\forall a \in \mathcal{A}$, arbitrarily, and $Q($terminal-state,$\cdot)=0$

Repeat (for each episode):

\quad\quad Initialize $S$

\quad\quad Repeat (for each step of episode):

\quad\quad \quad\quad Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)

\quad\quad \quad\quad Take action $A$, observe $R$, $S'$

\quad\quad \quad\quad $Q(S_t,A_t)\leftarrow Q(S_t,A_t)+ \alpha [ R_{t+1} + \gamma \max \limits_{a}Q(S_{t+1},a)-Q(S_t,A_t)]$

\quad\quad \quad\quad $S\leftarrow S'$

\quad\quad Until $S$ is terminal
}}

\end{center}

At each step, the agent will choose an action A according to current policy, and get the reward R. But the action value Q will be updated using another policy, here we choose the action which can maximize the next action value. That's why Q-learning is an off-policy algorithm, the policy we use to choose action and the policy we use to evaluate the value function are different. Here we prefer to the off-policy rather than on-policy algorithm, because off-policy algorithms are usually more efficient than on-policy.

\section{Simulation}
The goal of our simulation is to simulate the real traffic network. The e-puck will take on a role of explorer to find the shortest path between two points of the map.

To make our tasks tractable, we consider a grid-based representation with discrete states and action. The navigational task for our mobile robots could be projected into this presentation by employing a number of actions such as "turn left", "turn right", and "drive forward" that only use a low-level controller that takes care of accelerating, moving, and stopping. The different crossroad in the simulated map will be considered as states. We will perform our experiment in the simulator v-rap with our test map, which has 10 states.

In reinforcement learning, the desired behavior is implicitly specified by the reward function. The goal of our reinforcement learning algorithm is to find the optimized trajectory, which can maximize the accumulated long-term reward. In our experiment, we assign a reward '+10' at the destination state and '-1' at the other states. Also at the 'blind alley' it will get a very
poor reward such as '-10' in order to let agent avoid those states.
\begin{figure}[tb]
\centering 
\includegraphics[width=0.9\columnwidth]{simulatemap} 
\caption[An example of a floating figure]{An example of simulation map} % The text in the square bracket is the caption for the list of figures while the text in the curly brackets is the figure caption
\label{fig:gallery} 
\end{figure}
At the begin of each simulation, two points will set up as the start point and destination, and the e-puck robot will be placed in the start point of the test map, and start to explore the test map. From one state to adjacent state, the robot will drive along the line we drawn on the ground. The under sensor on e-puck will scan the ground all the time in order to keep robot following the line and detect the label we use to distinguish state. Everytime the robot reaches a new state, the Q-value will be updated with the reward we received and a new action will be selected according to the current policy. Once e-puck reaches the terminal state(destination), it will be replaced in the same start point and start a new exploration. Through trial and error, finally the action value will converge at a point, which is called optimal action value, then we can find the optimal policy for this problem.
Figure~\vref{fig:gallery} 

\section{Milestones}
to be written
to be written




%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{unsrt}

\bibliography{sample}

%----------------------------------------------------------------------------------------

\end{document}